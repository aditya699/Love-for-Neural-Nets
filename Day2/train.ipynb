{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other findings for the day.\n",
    "# We can now train a reasoning model using https://unsloth.ai/blog/r1-reasoning, with a single  7GB VRAM GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.314854</td>\n",
       "      <td>-0.315803</td>\n",
       "      <td>0.482772</td>\n",
       "      <td>0.406596</td>\n",
       "      <td>0.651483</td>\n",
       "      <td>-0.514341</td>\n",
       "      <td>0.287409</td>\n",
       "      <td>0.361406</td>\n",
       "      <td>0.113588</td>\n",
       "      <td>-0.435126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024764</td>\n",
       "      <td>-0.214943</td>\n",
       "      <td>-0.171342</td>\n",
       "      <td>0.197776</td>\n",
       "      <td>-0.088101</td>\n",
       "      <td>0.018636</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>-0.015001</td>\n",
       "      <td>-0.348425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151626</td>\n",
       "      <td>-0.027887</td>\n",
       "      <td>0.407615</td>\n",
       "      <td>0.026858</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>-0.376765</td>\n",
       "      <td>0.378584</td>\n",
       "      <td>0.799385</td>\n",
       "      <td>-0.485622</td>\n",
       "      <td>-0.581311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189612</td>\n",
       "      <td>-0.078242</td>\n",
       "      <td>-0.092983</td>\n",
       "      <td>0.178424</td>\n",
       "      <td>-0.132592</td>\n",
       "      <td>-0.271396</td>\n",
       "      <td>-0.030238</td>\n",
       "      <td>0.402832</td>\n",
       "      <td>-0.168941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530745</td>\n",
       "      <td>0.066995</td>\n",
       "      <td>0.319736</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>0.396067</td>\n",
       "      <td>-0.457637</td>\n",
       "      <td>0.494592</td>\n",
       "      <td>0.412954</td>\n",
       "      <td>-0.056115</td>\n",
       "      <td>-0.468745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>-0.090966</td>\n",
       "      <td>-0.280942</td>\n",
       "      <td>-0.050126</td>\n",
       "      <td>-0.263792</td>\n",
       "      <td>-0.210276</td>\n",
       "      <td>-0.273926</td>\n",
       "      <td>0.368495</td>\n",
       "      <td>-0.379933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.469785</td>\n",
       "      <td>-0.187990</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>-0.062842</td>\n",
       "      <td>0.632343</td>\n",
       "      <td>-0.298325</td>\n",
       "      <td>0.766535</td>\n",
       "      <td>0.998686</td>\n",
       "      <td>-0.244634</td>\n",
       "      <td>0.203218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067121</td>\n",
       "      <td>-0.180987</td>\n",
       "      <td>-0.703800</td>\n",
       "      <td>0.086969</td>\n",
       "      <td>-0.030878</td>\n",
       "      <td>-0.199462</td>\n",
       "      <td>-0.478008</td>\n",
       "      <td>-0.176455</td>\n",
       "      <td>0.067534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.206392</td>\n",
       "      <td>0.141666</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.033523</td>\n",
       "      <td>0.189142</td>\n",
       "      <td>-0.165017</td>\n",
       "      <td>0.106561</td>\n",
       "      <td>0.311256</td>\n",
       "      <td>0.091679</td>\n",
       "      <td>-0.673464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270695</td>\n",
       "      <td>-0.048474</td>\n",
       "      <td>-0.252568</td>\n",
       "      <td>-0.305456</td>\n",
       "      <td>-0.062712</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.275179</td>\n",
       "      <td>-0.087495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.314854 -0.315803  0.482772  0.406596  0.651483 -0.514341  0.287409   \n",
       "1  0.151626 -0.027887  0.407615  0.026858  0.002005 -0.376765  0.378584   \n",
       "2  0.530745  0.066995  0.319736  0.006169  0.396067 -0.457637  0.494592   \n",
       "3  0.469785 -0.187990 -0.036101 -0.062842  0.632343 -0.298325  0.766535   \n",
       "4  0.206392  0.141666  0.001321  0.033523  0.189142 -0.165017  0.106561   \n",
       "\n",
       "          7         8         9  ...       759       760       761       762  \\\n",
       "0  0.361406  0.113588 -0.435126  ... -0.024764 -0.214943 -0.171342  0.197776   \n",
       "1  0.799385 -0.485622 -0.581311  ...  0.189612 -0.078242 -0.092983  0.178424   \n",
       "2  0.412954 -0.056115 -0.468745  ...  0.007627 -0.090966 -0.280942 -0.050126   \n",
       "3  0.998686 -0.244634  0.203218  ... -0.067121 -0.180987 -0.703800  0.086969   \n",
       "4  0.311256  0.091679 -0.673464  ...  0.270695 -0.048474 -0.252568 -0.305456   \n",
       "\n",
       "        763       764       765       766       767  label  \n",
       "0 -0.088101  0.018636  0.041526 -0.015001 -0.348425      0  \n",
       "1 -0.132592 -0.271396 -0.030238  0.402832 -0.168941      0  \n",
       "2 -0.263792 -0.210276 -0.273926  0.368495 -0.379933      0  \n",
       "3 -0.030878 -0.199462 -0.478008 -0.176455  0.067534      1  \n",
       "4 -0.062712 -0.016795  0.013259  0.275179 -0.087495      0  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Day 2: Training a simple neural net\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv(\"../Day1/bert_embeddings.csv\")\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic forward pass i.e actication(WX)+bias , it's just weights be learned during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1, 2, 3])\n",
      "Weights: tensor([-0.0696,  0.6583,  1.0663])\n",
      "Bias: tensor([1.2171])\n",
      "Weighted sum: tensor([5.6628])\n",
      "Output after ReLU: tensor([5.6628])\n"
     ]
    }
   ],
   "source": [
    "# A quick revision for the forward pass of the data.\n",
    "import torch\n",
    "x=torch.tensor([1,2,3])\n",
    "#Everything that happens inside pytorch is done using tensors.\n",
    "weights=torch.randn(3)\n",
    "bias=torch.randn(1)\n",
    "\n",
    "weighted_sum=torch.sum(x*weights)+bias\n",
    "output=torch.relu(weighted_sum)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "print(\"Weighted sum:\", weighted_sum)\n",
    "print(\"Output after ReLU:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3.])\n",
      "\n",
      "Layer weights (4 neurons, 3 weights each):\n",
      " tensor([[ 1.2774, -1.9667,  1.7751],\n",
      "        [-0.6306, -0.9692, -1.5649],\n",
      "        [ 0.1256, -0.2905,  1.3283],\n",
      "        [-0.1064,  0.7906, -0.2822]])\n",
      "\n",
      "Layer biases: tensor([ 1.0052, -0.4294,  0.7263,  0.4101])\n",
      "\n",
      "Outputs from each neuron: tensor([3.6744, 0.0000, 4.2557, 1.0384])\n",
      "\n",
      "Neuron 1 detailed computation:\n",
      "Weights: tensor([ 1.2774, -1.9667,  1.7751])\n",
      "Bias: 1.0052\n",
      "Output: 3.6744\n",
      "\n",
      "Neuron 2 detailed computation:\n",
      "Weights: tensor([-0.6306, -0.9692, -1.5649])\n",
      "Bias: -0.4294\n",
      "Output: 0.0000\n",
      "\n",
      "Neuron 3 detailed computation:\n",
      "Weights: tensor([ 0.1256, -0.2905,  1.3283])\n",
      "Bias: 0.7263\n",
      "Output: 4.2557\n",
      "\n",
      "Neuron 4 detailed computation:\n",
      "Weights: tensor([-0.1064,  0.7906, -0.2822])\n",
      "Bias: 0.4101\n",
      "Output: 1.0384\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Input features\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Let's create a layer with 4 neurons\n",
    "# Each neuron needs weights for each input (3 inputs in this case)\n",
    "layer_weights = torch.randn(4, 3)  # 4 neurons x 3 inputs\n",
    "layer_biases = torch.randn(4)      # 1 bias per neuron\n",
    "\n",
    "# Forward pass through the layer\n",
    "# matmul does the weighted sum for all neurons at once\n",
    "weighted_sums = torch.matmul(layer_weights, x) + layer_biases\n",
    "layer_outputs = torch.relu(weighted_sums)\n",
    "\n",
    "print(\"Input:\", x)\n",
    "print(\"\\nLayer weights (4 neurons, 3 weights each):\\n\", layer_weights)\n",
    "print(\"\\nLayer biases:\", layer_biases)\n",
    "print(\"\\nOutputs from each neuron:\", layer_outputs)\n",
    "\n",
    "# Let's see what each neuron computed individually\n",
    "for i in range(4):\n",
    "    neuron_weights = layer_weights[i]\n",
    "    neuron_bias = layer_biases[i]\n",
    "    neuron_output = torch.relu(torch.sum(neuron_weights * x) + neuron_bias)\n",
    "    print(f\"\\nNeuron {i+1} detailed computation:\")\n",
    "    print(f\"Weights: {neuron_weights}\")\n",
    "    print(f\"Bias: {neuron_bias:.4f}\")\n",
    "    print(f\"Output: {neuron_output:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179, 768])\n",
      "tensor([[ 0.3149, -0.3158,  0.4828,  ...,  0.0415, -0.0150, -0.3484],\n",
      "        [ 0.1516, -0.0279,  0.4076,  ..., -0.0302,  0.4028, -0.1689],\n",
      "        [ 0.5307,  0.0670,  0.3197,  ..., -0.2739,  0.3685, -0.3799],\n",
      "        ...,\n",
      "        [ 0.4713, -0.2122,  0.2477,  ...,  0.0108,  0.0847, -0.3637],\n",
      "        [ 0.1620, -0.3751,  0.4522,  ...,  0.0424, -0.0872, -0.0646],\n",
      "        [ 0.1286, -0.4601,  0.3153,  ...,  0.1095,  0.1296, -0.1109]])\n",
      "torch.Size([179])\n",
      "tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Setup X and Y\n",
    "X=torch.tensor(data.iloc[:,:-1].values,dtype=torch.float32)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "y=torch.tensor(data.iloc[:,-1].values,dtype=torch.float32)\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shapes:\n",
      "First layer: torch.Size([768, 64])\n",
      "Second layer: torch.Size([64, 1])\n",
      "\n",
      "Bias shapes:\n",
      "First layer: torch.Size([64])\n",
      "Second layer: torch.Size([1])\n",
      "\n",
      "Input sample shape: torch.Size([768])\n",
      "After first layer: torch.Size([64])\n",
      "Final output shape: torch.Size([1])\n",
      "Predicted probability: 0.5000473260879517\n",
      "\n",
      "Testing with batch:\n",
      "Input batch shape: torch.Size([5, 768])\n",
      "After first layer: torch.Size([5, 64])\n",
      "Final output shape: torch.Size([5, 1])\n",
      "Predicted probabilities for batch: tensor([0.5000, 0.4992, 0.4992, 0.4976, 0.4988])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Define network architecture\n",
    "input_size = 768    # Size of BERT embeddings\n",
    "hidden_size = 64    # Size of hidden layer\n",
    "output_size = 1     # Binary classification\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 2. Initialize weights and biases with correct shapes\n",
    "# First layer (input â†’ hidden)\n",
    "weights1 = torch.randn(input_size, hidden_size) * 0.01  # Shape: (768, 64)\n",
    "bias1 = torch.zeros(hidden_size)                        # Shape: (64)\n",
    "\n",
    "# Second layer (hidden â†’ output)\n",
    "weights2 = torch.randn(hidden_size, output_size) * 0.01 # Shape: (64, 1)\n",
    "bias2 = torch.zeros(output_size)                        # Shape: (1)\n",
    "\n",
    "# Let's inspect our network parameters\n",
    "print(\"Weights shapes:\")\n",
    "print(\"First layer:\", weights1.shape)\n",
    "print(\"Second layer:\", weights2.shape)\n",
    "print(\"\\nBias shapes:\")\n",
    "print(\"First layer:\", bias1.shape)\n",
    "print(\"Second layer:\", bias2.shape)\n",
    "\n",
    "# 3. Test forward pass with one sample\n",
    "sample = X[0]  # Take first sample\n",
    "print(\"\\nInput sample shape:\", sample.shape)  # Should be (768,)\n",
    "\n",
    "# Forward pass through first layer\n",
    "# sample needs to be (768,) to multiply with weights1 (768, 64)\n",
    "hidden = torch.matmul(sample, weights1) + bias1  # Result: (64,)\n",
    "hidden_activated = torch.relu(hidden)\n",
    "print(\"After first layer:\", hidden_activated.shape)  # Should be (64,)\n",
    "\n",
    "# Forward pass through second layer\n",
    "# hidden_activated (64,) multiplies with weights2 (64, 1)\n",
    "output = torch.matmul(hidden_activated, weights2) + bias2  # Result: (1,)\n",
    "final_output = torch.sigmoid(output)\n",
    "print(\"Final output shape:\", final_output.shape)\n",
    "print(\"Predicted probability:\", final_output.item())\n",
    "\n",
    "# Let's also test with a batch of samples\n",
    "batch = X[:5]  # Take first 5 samples\n",
    "print(\"\\nTesting with batch:\")\n",
    "print(\"Input batch shape:\", batch.shape)  # Should be (5, 768)\n",
    "\n",
    "# Forward pass with batch\n",
    "hidden_batch = torch.matmul(batch, weights1) + bias1  # Result: (5, 64)\n",
    "hidden_activated_batch = torch.relu(hidden_batch)\n",
    "print(\"After first layer:\", hidden_activated_batch.shape)\n",
    "\n",
    "output_batch = torch.matmul(hidden_activated_batch, weights2) + bias2  # Result: (5, 1)\n",
    "final_output_batch = torch.sigmoid(output_batch)\n",
    "print(\"Final output shape:\", final_output_batch.shape)\n",
    "print(\"Predicted probabilities for batch:\", final_output_batch.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "Loss: 0.6905\n",
      "Accuracy: 0.6006\n",
      "\n",
      "Epoch 10/100\n",
      "Loss: 0.2037\n",
      "Accuracy: 0.9652\n",
      "\n",
      "Epoch 20/100\n",
      "Loss: 0.0241\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 30/100\n",
      "Loss: 0.0100\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 40/100\n",
      "Loss: 0.0049\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 50/100\n",
      "Loss: 0.0029\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 60/100\n",
      "Loss: 0.0022\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 70/100\n",
      "Loss: 0.0017\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 80/100\n",
      "Loss: 0.0014\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Epoch 90/100\n",
      "Loss: 0.0012\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Training completed!\n",
      "Final Loss: 0.0011\n",
      "Final Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming we have our weights and biases from before\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Lists to store metrics\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    indices = torch.randperm(len(X))\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    \n",
    "    # Process mini-batches\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        # Get mini-batch\n",
    "        X_batch = X_shuffled[i:i + batch_size]\n",
    "        y_batch = y_shuffled[i:i + batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        # First layer\n",
    "        hidden = torch.matmul(X_batch, weights1) + bias1\n",
    "        hidden_activated = torch.relu(hidden)\n",
    "        \n",
    "        # Second layer\n",
    "        output = torch.matmul(hidden_activated, weights2) + bias2\n",
    "        predictions = torch.sigmoid(output)\n",
    "        \n",
    "        # Calculate loss (Binary Cross Entropy)\n",
    "        loss = -torch.mean(\n",
    "            y_batch.view(-1, 1) * torch.log(predictions + 1e-10) + \n",
    "            (1 - y_batch.view(-1, 1)) * torch.log(1 - predictions + 1e-10)\n",
    "        )\n",
    "        \n",
    "        # Backpropagation\n",
    "        # Output layer gradients\n",
    "        output_error = predictions - y_batch.view(-1, 1)\n",
    "        weights2_grad = torch.matmul(hidden_activated.t(), output_error)\n",
    "        bias2_grad = torch.sum(output_error, dim=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        hidden_error = torch.matmul(output_error, weights2.t())\n",
    "        hidden_error = hidden_error * (hidden_activated > 0).float()  # ReLU gradient\n",
    "        weights1_grad = torch.matmul(X_batch.t(), hidden_error)\n",
    "        bias1_grad = torch.sum(hidden_error, dim=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        weights1 -= learning_rate * weights1_grad\n",
    "        bias1 -= learning_rate * bias1_grad\n",
    "        weights2 -= learning_rate * weights2_grad\n",
    "        bias2 -= learning_rate * bias2_grad\n",
    "        \n",
    "        # Calculate accuracy for this batch\n",
    "        predicted_labels = (predictions >= 0.5).float()\n",
    "        accuracy = (predicted_labels == y_batch.view(-1, 1)).float().mean()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accuracies.append(accuracy.item())\n",
    "    \n",
    "    # Average metrics for this epoch\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    avg_accuracy = sum(epoch_accuracies) / len(epoch_accuracies)\n",
    "    losses.append(avg_loss)\n",
    "    accuracies.append(avg_accuracy)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Accuracy: {avg_accuracy:.4f}\\n\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final Loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {accuracies[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
