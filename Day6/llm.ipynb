{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Tokenize the dataset\n",
    "\n",
    "1.Tokenization enables conversion of text into id's (this id will be used to later convert the text back to the original text and in embedding models also)\n",
    "\n",
    "2.One can implement byte pair encoding from scratch but here we will use the GPT-2 tokenizer which is already implemented in tiktoken,\n",
    "\n",
    "2.1The tokenizer reads Unicode text.\n",
    "\n",
    "2.2Internally encodes it to byte sequences (as needed),\n",
    "\n",
    "2.3Then applies BPE merges to get tokens, and maps each token to a token ID.\n",
    "\n",
    "# Incase you want to implement BPE from scratch please refer to the following video https://www.youtube.com/watch?v=zduSFxRajkE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text sample:\n",
      " First Citizen:\n",
      "Before we proce\n",
      "\n",
      "Token IDs (first 10): [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11]\n",
      "\n",
      "Decoded (first 10 tokens):\n",
      " First Citizen:\n",
      "Before we proceed any further,\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Step 2: Load the GPT-2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Step 3: Encode the text using GPT-2 BPE tokenizer\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "# Step 4: Decode to verify\n",
    "decoded_text = tokenizer.decode(token_ids[:10])\n",
    "\n",
    "# Print results\n",
    "print(\"Original text sample:\\n\", text[:30])\n",
    "print(\"\\nToken IDs (first 10):\", token_ids[:10])\n",
    "print(\"\\nDecoded (first 10 tokens):\\n\", decoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
