{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# DAY 3 What made transformers so successful? like on a serious note the world is different now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "ðŸ“Œ Problem with Basic Seq2Seq Models (Without Attention)\n",
    "In a traditional Sequence-to-Sequence (Seq2Seq) model (like an RNN-based encoder-decoder), the encoder processes the input sequence and compresses it into a single fixed-size context vector (the last hidden state).\n",
    "\n",
    "ðŸš¨ Whatâ€™s the issue?\n",
    "\n",
    "This fixed-size context vector acts as a bottleneckâ€”it must store all the information from the input sequence, even if the sequence is very long.\n",
    "This leads to poor performance on long sentences, as the decoder struggles to retrieve earlier words.\n",
    "The model tends to forget earlier parts of the sentence when generating longer outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The basic RNN cell equation can be represented as follows:\n",
    "# h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
    "# where:\n",
    "# - h_t is the hidden state at time t\n",
    "# - h_{t-1} is the hidden state at time t-1\n",
    "# - x_t is the input at time t\n",
    "# - W_hh is the weight matrix for the hidden state\n",
    "# - W_xh is the weight matrix for the input\n",
    "# - b_h is the bias term\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#We are implementing a basic RNN cell in PyTorch. This will help us understand how an RNN processes a sequence over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[1 2]\n",
      " [2 3]\n",
      " [3 4]\n",
      " [4 5]\n",
      " [5 6]\n",
      " [6 7]\n",
      " [7 8]\n",
      " [8 9]]\n",
      "Targets:\n",
      " [ 3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a simple sequence of numbers\n",
    "sequence = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Prepare input-output pairs\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(sequence) - 2):  # We need at least 2 previous values\n",
    "    X.append([sequence[i], sequence[i+1]])  # Input is two previous numbers\n",
    "    y.append(sequence[i+2])  # Target is the next number\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Print dataset\n",
    "print(\"Inputs:\\n\", X)\n",
    "print(\"Targets:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 3],\n",
       "       [3, 4],\n",
       "       [4, 5],\n",
       "       [5, 6],\n",
       "       [6, 7],\n",
       "       [7, 8],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[1 2]\n",
      " [2 3]\n",
      " [3 4]\n",
      " [4 5]\n",
      " [5 6]\n",
      " [6 7]\n",
      " [7 8]\n",
      " [8 9]]\n",
      "Targets:\n",
      " [[ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate simple sequence data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9]])\n",
    "y = np.array([[3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "\n",
    "print(\"Inputs:\\n\", X)\n",
    "print(\"Targets:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (8, 2)\n",
      "Shape of y: (8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49671415]]\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "[[-0.1382643]]\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "[[0.64768854]]\n",
      "(1, 1)\n",
      "[[1.52302986]]\n",
      "(1, 1)\n",
      "[[1.14440269]]\n",
      "(1, 1)\n",
      "[[1.24650125]]\n",
      "(1, 1)\n",
      "[[0.98337764]]\n",
      "(1, 1)\n",
      "[[-0.23415337]]\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "[[-0.23413696]]\n",
      "(1, 1)\n",
      "[[-0.46439815]]\n",
      "(1, 1)\n",
      "[[-0.46439815]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1 Hidden Layer\n",
    "np.random.seed(42)\n",
    "w1=np.random.randn(1,1)\n",
    "print(w1)\n",
    "print(w1.shape)\n",
    "W1=w1.T\n",
    "print(W1.shape)\n",
    "w2=np.random.randn(1,1)\n",
    "print(w2)\n",
    "print(w2.shape)\n",
    "W2=w2.T\n",
    "print(W2.shape)\n",
    "b1=np.random.randn(1,1)\n",
    "print(b1)\n",
    "print(b1.shape)\n",
    "b2=np.random.randn(1,1)\n",
    "print(b2)\n",
    "print(b2.shape)\n",
    "\n",
    "\n",
    "\n",
    "#for the first layer input will be multiplied by W1 and then added to b1\n",
    "before_activation_1_1=((X[0][0]*W1)+b1)\n",
    "print(before_activation_1_1)\n",
    "print(before_activation_1_1.shape)\n",
    "before_activation_1_2=((X[0][1]*W2)+b2)\n",
    "print(before_activation_1_2)\n",
    "print(before_activation_1_2.shape)\n",
    "\n",
    "after_activation_1_full=np.tanh(before_activation_1_1 + before_activation_1_2)\n",
    "print(after_activation_1_full)\n",
    "print(after_activation_1_full.shape)\n",
    "\n",
    "W3=np.random.randn(1,1)\n",
    "print(W3)\n",
    "print(W3.shape)\n",
    "W3=W3.T\n",
    "print(W3.shape)\n",
    "b3=np.random.randn(1,1)\n",
    "print(b3)\n",
    "print(b3.shape)\n",
    "#final layer\n",
    "before_activation_2_1=((after_activation_1_full*W3)+b3)\n",
    "print(before_activation_2_1)\n",
    "print(before_activation_2_1.shape)\n",
    "\n",
    "#Let's not use any activation function for the final layer since it seems a regression problem\n",
    "after_activation_2_1=(before_activation_2_1)\n",
    "print(after_activation_2_1)\n",
    "print(after_activation_2_1.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
